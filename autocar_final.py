# -*- coding: utf-8 -*-
"""Autocar_FINAL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kztIntQL68KF80E5oDTzMDU4W_Z1neds

# ***IMPORT LIBRARIES & LOAD DATA***
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

data = pd.read_csv('data.csv')
data.head()

"""# ***ABOUT DATA***"""

#Dimension of the data
data.shape

data.info()

#Check if there is NaN values.
data.isna().sum()

#Transformation to the DATETIME
data['ORDERDATE'] = pd.to_datetime(data['ORDERDATE'])

#How many customers have order at last order date.
data[data['ORDERDATE'] == '2020-05-31']['CUSTOMERNAME'].nunique()

"""As we can see, there is some sword of compliaction because of the duplicating datas. This is why we'll creating new Days difference according to the ORDERDATE which it's also not
a very distinguishing feature.

"""

# Drop the 'DAYS_SINCE_LASTORDER' column
data = data.drop('DAYS_SINCE_LASTORDER', axis=1)

#Creating a new timeline
new_date = datetime(2020,6,1) - data['ORDERDATE']
#Adding new Days_Since_LastOrder column
data['DAYS_SINCE_LASTORDER'] = new_date.dt.days
data.head()

data['DAYS_SINCE_LASTORDER'].nunique()

"""It's not good to have 246 unique values, since there is 100k data. But we'll continue.

#***Data Charactristics***

*   Dataset has 100000 entries with 20 columns.

*   None of these columns have NaN values.

*   Because of duplicating the data there is some wrong duplications.

*   Data types vary across columns, including floats, datetime, and objects.
"""

data.head(2)

"""# ***CLEANING DATA***"""

#Drop the columns irrelevant at the EDA()
data = data.drop(columns=['PHONE','ADDRESSLINE1','POSTALCODE','CONTACTLASTNAME','CONTACTFIRSTNAME'])

data.head()

data_num = data.select_dtypes(include=['float64', 'int64']).drop(columns=['ORDERNUMBER'])
print(data_num.columns, len(list(data_num.columns.values)))

data_num.head()

display(round(data_num.describe(),2).T)

"""**Sales Amount**: The avg sales amount per transaction is approximately 3.553.
Customers make purchases ranging from '199.38' to '193348.50'.


**Quantity Ordered**: On avg, customers approximately buys 35 items per transaction with a minimum of 6 and a max of 97.


**Price Each**: The avarage price of each item in an order is approximately 101.
Prices vary between a minimum of 26 and max of 252.

**MSRP**: Manufacturer's Suggested Retail Price. The average MSRP is approximately 100, with prices ranging from 39 to 214.


"""

data_obj = data.select_dtypes(include = ['object'])
print(data_obj.columns, len(list(data_obj.columns.values)))

data_obj.head()

data_obj['STATUS'].unique()

#In Process, Resolved, Shipped, On Hold could be useful.
#But we need drop the Disputed and Cancalled columns
data_obj = data_obj[data_obj['STATUS'] != 'Disputed']
data_obj = data_obj[data_obj['STATUS'] != 'Cancelled']

#We dropped the unwanted columns.
data_obj['STATUS'].unique()

data_obj.select_dtypes(include = ['object']).describe().T

"""**Order Status**: The most common order status is "Shipped," occurring 92483 times.


**Product Code**: 'S32_2509' is the most popular product. But we cant use it in the model because it has very unique values. Hard to encoding, but low value to use.

**Product Line**: "Classic Cars" is the dominant product line with 33478 entries.


**Customer Concentration**: "Euro Shopping Channel" is the top customer, with 9042 transactions.


**Geographic Trends**: "Madrid" and "USA" are the most frequent city and country, respectively.


**Deal Sizes**: The majority of deals fall into the "Medium" category, accounting for 47668 cases.

# **TOUCHS**
"""

data_obj['PRODUCT_DEAL_COMBO'] = data_obj['DEALSIZE'] + '_' + data_obj['PRODUCTLINE']
data_obj['PRODUCT_DEAL_COMBO'].value_counts()

combo_encoded = pd.get_dummies(data_obj['PRODUCT_DEAL_COMBO'], prefix='PD')
data_obj = pd.concat([data_obj, combo_encoded], axis=1)

data_obj

"""# ***VISUALIZATION***

**DATA OBJECT DISTRIBUTION**
"""

data_obj['PRODUCTLINE'].value_counts().plot.pie(autopct='%1.1f%%', figsize=(6, 6), title='DEALSIZE DISTRIBUTION ON PIE')
plt.ylabel('')
plt.show()

data_obj['PRODUCTLINE'].value_counts().plot(kind='bar', color='purple', title='PRODUCTLINE DISTRIBUTION ON BAR')
plt.xlabel('Values')
plt.ylabel('Categories')
plt.show()

data_obj['DEALSIZE'].value_counts().plot(kind='bar', color='skyblue', title='DEALSIZE DISTRIBUTION')
plt.xlabel('DEALSIZE')
plt.ylabel('COUNT')
plt.show()

#Cleaned Status Data
data_obj['STATUS'].value_counts().plot(kind='bar', color='orange', title='STATUS DISTRIBUTION')
plt.xlabel('STATUS')
plt.ylabel('COUNT')
plt.show()

"""There could be outputs as an active customers in the future analysis."""

data_obj['COUNTRY'].value_counts().head(10).plot(kind='bar',color = 'green', title='TOP 10 COUNTRY')
plt.xlabel('COUNTRY')
plt.ylabel('COUNT')
plt.show()

"""As we can see, USA is the country that has bought most.

*Model can be ask if the buyers from USA or not.*
"""

data_obj['CITY'].value_counts().head(10).plot(kind='bar',color = 'yellow', title='TOP 10 CITY')
plt.xlabel('CITY')
plt.ylabel('COUNT')
plt.show()

"""Despite the USA, there is  2 country at the top from the Spain.


Couple cities at Spain has big spending habits.

**DATA NUM DISTRIBUTION**
"""

#We had couple drop's at the data_obj df. We had to use same indexes at data_num.
data_num = data_num.loc[data_obj.index]

data_num.info()

# Plotting the all numerical columns.
for column in data_num.columns:
    print(f"➤ {column} sütunu analizi")
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    #Creating Histogram
    sns.histplot(data_num[column], bins=30, kde=True, ax=axes[0], color='skyblue')
    axes[0].set_title(f"{column} - Histogram")
    axes[0].set_xlabel(column)

    # Boxplot - minimum, maksimum, median, quartiles and outliers
    sns.boxplot(x=data_num[column], ax=axes[1], color='lightgreen')
    axes[1].set_title(f"{column} - Boxplot")

    plt.tight_layout()
    plt.show()

"""***QUANTITYORDERED***:

*Histogram*:
*   Values mostly between 20-50.

*   There is couple outliers but rare.

*Boxplot*:

*   The median is approximately 35.


*   Above 60+ may be considered an outlier.

**90+ Quantity Ordered might has meaningful on person analysis.**


---


***PRICEEACH***

*Histogram*:
*   Values mostly between 30-100.

*   There is couple 100+ price outliers but rare.

*Boxplot*:

*  Lots of outliers but they might be special collections. Must scan.


---


***ORDERLINENUMBER***

*Histogram*:
*   The most frequent values ​​are 1–6 meaning that most orders have few products.

*   The characteristics of the value decrease with frequency.


*Boxplot*:

*   Has right tail but no outliers.

**Fewer items per order. Might be personel customer mucher.**


---
***SALES***

*Histogram*:
*   Skewed histogram.

*   Low sales more than big sales.

*Boxplot*:

*   Too long tail.(Too much outliers)


**Big sales are rare but important. It must be scaled while going to be segmentation.**

---
***MSRP***

*Histogram*:
*   70-130 High Frequency.

*   Multiple tops.

*Boxplot*:

*   200+ Outliers.
---
***DAYS_SINCE_LAST_ORDER***

*Histogram*:
*   There is ordered jumps such as 200,400,600.

*   This may indicate that orders are placed at certain periods.


*Boxplot*:

*   Uneven distribution
"""

# Calculate Correlation
corr = data_num.corr()

# Upper Triangle
import numpy as np
mask = np.triu(np.ones_like(corr, dtype=bool))

plt.figure(figsize=(10, 6))
sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', mask=mask, square=True, linewidths=0.5)
plt.title("Correlation Matris Between Numerical Columns")
plt.show()

"""**SALES & PRICEEACH**

0.81 Very strong positive relationship → If price increases, sales increase (probably because quantity remains constant and price increases).

**SALES & QUANTITYORDERED:**

0.54 Medium level relationship → The more orders, the more sales.


**All other values ​**:

​≈ 0 No significant relationship → Independence is high.

"""



"""# ***FEATURE ENGINEERING***

# ***Time Bases Analysis***
"""

data

#Time Analysis based on original data.copy
data = data.loc[data_obj.index]
data.info()

#For saving the original data.
data_time = data.copy()

#Creating Time Series.
data_time['YEAR'] = data_time['ORDERDATE'].dt.year
data_time['MONTH'] = data_time['ORDERDATE'].dt.month
data_time['WEEK'] = data_time['ORDERDATE'].dt.isocalendar().week
data_time['QUARTER'] = data_time['ORDERDATE'].dt.quarter

data_time

yearly_sales = data_time.groupby('YEAR')['SALES'].sum()

yearly_sales.plot(kind='bar', figsize=(10, 6))
plt.title('Yearly Total Sales')
plt.ylabel('Total Sales')
plt.xlabel('Year')
plt.grid(True)
plt.show()

"""Most productivity at sales in 2019."""

monthly_sales = data_time.groupby('MONTH')['SALES'].mean()

monthly_sales.plot(kind='line', marker='o', figsize=(10, 5))
plt.title('Average Monthly Sales Across All Years')
plt.ylabel('Average Sales')
plt.xlabel('Month')
plt.xticks(range(1, 13))
plt.grid(True)
plt.show()

"""We can say avarage sales per order are going highest  at the summer. So we can say there may be a seasonal shopping effect here.

"""

monthly_yearly = data.groupby([data['ORDERDATE'].dt.to_period('M')])['SALES'].sum()

monthly_yearly.plot(figsize=(12, 6))
plt.title('Monthly Sales Trend Over Time')
plt.ylabel('Sales')
plt.xlabel('Month-Year')
plt.grid(True)
plt.xticks(rotation=45)
plt.show()

"""This graph shows:

“How many sales were made (total) in each month of each year?”

So:
November 2018 : 37 million sales
June 2018 : 10 million sales, the differences are very noticeable

**So there might be a event such as BLACK FRIDAY at November. This must include at the model.**
"""

quarter_avg = data_time.groupby('QUARTER')['SALES'].mean()

quarter_avg.plot(kind='bar', color='lightcoral', figsize=(8,4))
plt.title("Çeyrek Bazlı Ortalama Satışlar")
plt.xlabel("Çeyrek")
plt.ylabel("Ortalama Satış")
plt.xticks([0,1,2,3], ['Q1','Q2','Q3','Q4'])
plt.show()

"""**We cant have any further data from Quarters. They're even.**"""

heatmap_data = data_time.groupby(['YEAR', 'MONTH'])['SALES'].sum().unstack()
sns.heatmap(heatmap_data, cmap="YlGnBu", annot=False, fmt=".0f")
plt.title("Sales Heatmap by Year and Month")
plt.show()

"""**As we can see it here same with the Monthly-Yearly graph. This heatmaps shows there is a very big sales at the 11. Month.**



*And also we can see there is a pattern at Q4. So we can also use it.*
"""

# Haftanın günlerini çıkar
data_time['WEEKDAY'] = data_time['ORDERDATE'].dt.dayofweek  # 0 = Pazartesi, 6 = Pazar

# Gün isimlerini de ekle (isteğe bağlı)
data_time['WEEKDAY_NAME'] = data_time['ORDERDATE'].dt.day_name()

# Günlere göre ortalama satış
weekday_avg = data_time.groupby('WEEKDAY_NAME')['SALES'].mean().sort_values(ascending=False)

# Çizim
weekday_avg.plot(kind='bar', color='slateblue', figsize=(8,5))
plt.title("Haftanın Günlerine Göre Ortalama Satışlar")
plt.xlabel("Gün")
plt.ylabel("Ortalama Satış")
plt.xticks(rotation=45)
plt.show()

"""There is no patterns in the Weekdays."""

data_time['DAY_OF_MONTH'] = data_time['ORDERDATE'].dt.day

day_avg = data_time.groupby('DAY_OF_MONTH')['SALES'].mean()

day_avg.plot(kind='line', figsize=(12,5), marker='o')
plt.title("Ayın Günlerine Göre Ortalama Satış")
plt.xlabel("Gün")
plt.ylabel("Ortalama Satış")
plt.grid(True)
plt.show()

"""These graph shows also dont have patterns in the month of days(1-31).


Graph seems like there is a difference but when we look closely, avarage sales changes between 3500-3600. Its approximately %3.


It could be a noise for our model is why we're not gonna use it.
"""

# Adding useable features at the model.
features_time = pd.DataFrame(index=data.index)
features_time['IS_Q4'] = data['ORDERDATE'].dt.quarter.apply(lambda x: 1 if x == 4 else 0)
features_time['IS_NOV'] = data['ORDERDATE'].dt.month.apply(lambda x: 1 if x == 11 else 0)
features_time['IS_SUMMER_PEAK'] = data['ORDERDATE'].dt.month.apply(lambda x: 1 if x in [5,6,7] else 0)

features_time

"""# ***Behavior Based Analysis***"""

data_obj

combo_df = pd.concat([data['SALES'], data_obj['PRODUCT_DEAL_COMBO']], axis=1)

combo_summary = combo_df.groupby('PRODUCT_DEAL_COMBO')['SALES'].agg(['mean', 'sum', 'count']).sort_values('sum', ascending=False)
combo_summary.rename(columns={'mean': 'AVG_SALES', 'sum': 'TOTAL_SALES', 'count': 'ORDER_COUNT'}, inplace=True)

display(combo_summary)

""""Classic Cars and Vintage Cars have the highest total sales.
This means they are the most popular product lines across all deal sizes."



"Most top-selling combos are Medium deal sizes, which suggests that
customers often choose a middle-sized order instead of going very big or small."



"Average sales per order are around 3500 in all combos,
which means customers spend a similar amount regardless of product or deal size."



"Some combos like 'Large_Classic Cars' or 'Large_Motorcycles'
have fewer orders but still generate high total sales.
This indicates premium or high-value transactions by fewer customers."





"'Medium_Classic Cars' and 'Small_Classic Cars' are not only top in total sales,
but also have high order counts. This makes them key market segments."



"""

import matplotlib.pyplot as plt

# En yüksek toplam satış getiren ilk 10 kombinasyon
top_combos = combo_summary.head(10)

plt.figure(figsize=(12,6))
top_combos['TOTAL_SALES'].plot(kind='bar', color='coral')
plt.title('Top 10 PRODUCT_DEAL_COMBO by Total Sales')
plt.ylabel('Total Sales')
plt.xlabel('Product & DealSize Combo')
plt.xticks(rotation=45)
plt.grid(True)
plt.tight_layout()
plt.show()

"""**In conclusion, average sales are stable, but total sales and order count reveal different customer behaviors.**


**Some customers prefer frequent purchases, while others spend a lot with fewer transactions.
These patterns can help us create meaningful customer segments.**
"""

features_behavior = pd.DataFrame(index=data.index)

#Most bought product.
features_behavior['IS_CLASSIC_OR_VINTAGE'] = data['PRODUCTLINE'].apply(
    lambda x: 1 if x in ['Classic Cars', 'Vintage Cars'] else 0
)

#If it's a luxury spending, least common but same avg. spending.

premium_combos = ['Large_Classic Cars', 'Large_Motorcycles']

features_behavior['IS_LUXURY'] = data_obj['PRODUCT_DEAL_COMBO'].apply(
    lambda x: 1 if x in premium_combos else 0
)

features_behavior

features_behavior['IS_LUXURY'].value_counts()

# 1. En çok kazandıran 3 combo'yu bul
top_combos = combo_df.groupby('PRODUCT_DEAL_COMBO')['SALES'].sum().sort_values(ascending=False).head(3).index.tolist()

# 2. Feature olarak ata
features_behavior['IS_TOP_COMBO'] = combo_df['PRODUCT_DEAL_COMBO'].apply(lambda x: 1 if x in top_combos else 0)

features_behavior['IS_TOP_COMBO'].value_counts()

features_behavior

#Noticed first column and last column has similiar values.
#So testing corr. between them.
features_behavior.corr()['IS_CLASSIC_OR_VINTAGE']['IS_TOP_COMBO']

"""This means, they have strong relationship but there could be differences lets keep them both."""

threshold_qty = data['QUANTITYORDERED'].quantile(0.75)  # %75 üzerini heavy olarak tanımla

features_behavior['IS_HEAVY_ORDER'] = (data['QUANTITYORDERED'] > threshold_qty).astype(int)

features_behavior['IS_HEAVY_ORDER'].value_counts()

threshold_value = data['SALES'].quantile(0.75)  # %75 üzerini yüksek satış olarak tanımla

features_behavior['IS_HIGH_VALUE_ORDER'] = (data['SALES'] > threshold_value).astype(int)

features_behavior['IS_HIGH_VALUE_ORDER'].value_counts()

overlap = features_behavior[
    (features_behavior['IS_HIGH_VALUE_ORDER'] == 1) &
    (features_behavior['IS_HEAVY_ORDER'] == 1)
]

print(f"Her ikisi de 1 olan satır sayısı: {len(overlap)}")
print(f"Oran: {len(overlap) / len(features_behavior):.2%}")

features_behavior[['IS_HIGH_VALUE_ORDER', 'IS_HEAVY_ORDER']].corr()

"""# ***Customer Based Analysis***"""

# 1. Müşteri başına toplam sipariş sayısı
total_orders = data.groupby('CUSTOMERNAME')['ORDERNUMBER'].nunique()

# 2. Müşteri başına toplam harcama
total_sales = data.groupby('CUSTOMERNAME')['SALES'].sum()

# 3. Ortalama sipariş başı harcama (Total Sales / Total Orders)
avg_order_value = total_sales / total_orders

# 4. DataFrame'e ekle
features_customer_segment = pd.DataFrame({
    'TOTAL_ORDERS': total_orders,
    'TOTAL_SALES': total_sales,
    'AVG_ORDER_VALUE': avg_order_value
})

features_customer_segment.describe()

features_customer = pd.DataFrame(index=data['CUSTOMERNAME'].unique())

threshold_sales = features_customer_segment['TOTAL_SALES'].quantile(0.80)
features_customer['IS_TOP_CUSTOMER'] = (features_customer_segment['TOTAL_SALES'] > threshold_sales).astype(int)

features_customer_segment

threshold_sales = features_customer_segment['TOTAL_SALES'].quantile(0.80)
features_customer['IS_TOP_CUSTOMER'] = (features_customer_segment['TOTAL_SALES'] > threshold_sales).astype(int)

features_customer[features_customer==1].isna().sum()

customer_combo_matrix = pd.crosstab(
    index=data['CUSTOMERNAME'],
    columns=data_obj['PRODUCT_DEAL_COMBO'],
    values=data['QUANTITYORDERED'],
    aggfunc='sum'
).fillna(0)

customer_combo_matrix

features_customer_combo = pd.DataFrame(index=customer_combo_matrix.index)

features_customer_combo

# Classic / Vintage / Medium / Small combo sütunlarını filtrele
classic_cols = [col for col in customer_combo_matrix.columns if 'Classic' in col]
vintage_cols = [col for col in customer_combo_matrix.columns if 'Vintage' in col]
medium_cols = [col for col in customer_combo_matrix.columns if 'Medium' in col]
small_cols = [col for col in customer_combo_matrix.columns if 'Small' in col]

# Toplam siparişe oranla %25'ten fazlası olanları 1 olarak işaretle
features_customer_combo = pd.DataFrame(index=customer_combo_matrix.index)

features_customer_combo['IS_CLASSIC_FOCUSED'] = (
    customer_combo_matrix[classic_cols].sum(axis=1) / customer_combo_matrix.sum(axis=1) > 0.25
).astype(int)

features_customer_combo['IS_VINTAGE_FOCUSED'] = (
    customer_combo_matrix[vintage_cols].sum(axis=1) / customer_combo_matrix.sum(axis=1) > 0.25
).astype(int)

features_customer_combo['IS_MEDIUM_FOCUSED'] = (
    customer_combo_matrix[medium_cols].sum(axis=1) / customer_combo_matrix.sum(axis=1) > 0.25
).astype(int)

features_customer_combo['IS_SMALL_FOCUSED'] = (
    customer_combo_matrix[small_cols].sum(axis=1) / customer_combo_matrix.sum(axis=1) > 0.25
).astype(int)

import matplotlib.pyplot as plt
import seaborn as sns

# Sütun bazında kaç kişi için 1 olduğu
df_plot = features_customer_combo.sum().reset_index()
df_plot.columns = ['Feature', 'Count']

plt.figure(figsize=(8,5))
sns.barplot(data=df_plot, x='Feature', y='Count', palette='Set2')
plt.title("Lack of Variation in Customer Combo Preferences")
plt.ylabel("Number of Customers with Value = 1")
plt.xlabel("Feature")
plt.xticks(rotation=30)
plt.tight_layout()
plt.show()

"""Note: After conducting extensive behavioral analysis, I found that most customers exhibit very similar shopping habits.


Specifically:


	•	A majority favor Classic and Vintage Cars.
	•	Most purchases are concentrated in Small and Medium deal sizes.


  

Due to this uniformity, these customer-level features do not contribute significantly to differentiation or segmentation.
Therefore, I will proceed with the region-based analysis to explore potential patterns across geographic segments.
"""



"""# ***Region Based Analysis***"""

CITY_FREQ = data['CITY'].value_counts()
COUNTRY_FREQ = data['COUNTRY'].value_counts()

import matplotlib.pyplot as plt
import seaborn as sns

# İlk 10 şehir
top_cities = data['CITY'].value_counts().head(10)

plt.figure(figsize=(10,6))
sns.barplot(x=top_cities.values, y=top_cities.index, palette='Blues_r')
plt.title('Top 10 Cities by Order Frequency')
plt.xlabel('Number of Orders')
plt.ylabel('City')
plt.tight_layout()
plt.show()

# İlk 10 ülke için aynısı
top_countries = data['COUNTRY'].value_counts().head(10)

plt.figure(figsize=(10,6))
sns.barplot(x=top_countries.values, y=top_countries.index, palette='Greens_r')
plt.title('Top 10 Countries by Order Frequency')
plt.xlabel('Number of Orders')
plt.ylabel('Country')
plt.tight_layout()
plt.show()

city_threshold = data['CITY'].value_counts().quantile(0.75)
country_threshold = data['COUNTRY'].value_counts().quantile(0.75)

features_region = pd.DataFrame(index=data.index)
features_region['IS_BIG_CITY'] = data['CITY'].map(data['CITY'].value_counts() > city_threshold).fillna(False).astype(int)
features_region['IS_BIG_COUNTRY'] = data['COUNTRY'].map(data['COUNTRY'].value_counts() > country_threshold).fillna(False).astype(int)

features_region

city_avg_sales = data.groupby('CITY')['SALES'].mean().sort_values(ascending=False)
city_avg_sales.head(10)  # En yüksek ortalama satışa sahip 10 şehir

country_product_counts = data.pivot_table(index='COUNTRY', columns='PRODUCTLINE',
                                          values='SALES', aggfunc='count', fill_value=0)
country_product_counts

country_pref = data.groupby(['COUNTRY', 'PRODUCTLINE'])['SALES'].count().unstack().fillna(0)
country_pref = country_pref.div(country_pref.sum(axis=1), axis=0)  # oranları almak için
country_pref

# Önce ilgili sütunları birleştirelim
luxury_df = pd.concat([data[['CITY']], features_behavior['IS_LUXURY'], features_region['IS_BIG_CITY']], axis=1)

# Sadece büyük şehirleri filtrele
big_city_luxury = luxury_df[luxury_df['IS_BIG_CITY'] == 1]

# Büyük şehirlerde lüks alışveriş oranı
luxury_ratio = big_city_luxury['IS_LUXURY'].mean()
print(f"Büyük şehirlerde lüks alışveriş oranı: {luxury_ratio:.2%}")

small_city_luxury = luxury_df[luxury_df['IS_BIG_CITY'] == 0]
small_luxury_ratio = small_city_luxury['IS_LUXURY'].mean()
print(f"Küçük şehirlerde lüks alışveriş oranı: {small_luxury_ratio:.2%}")

country_avg_sales = data.groupby('COUNTRY')['SALES'].mean().sort_values(ascending=False)

country_avg_sales

country_pref = data.groupby(['COUNTRY', 'PRODUCTLINE'])['SALES'].count().unstack()
country_pref = country_pref.div(country_pref.sum(axis=1), axis=0)

country_pref

"""“Despite extensive analysis across regions, no significant behavioral difference was observed. Most cities and countries exhibited highly similar average order values and product preferences. Therefore, while raw city/country columns were not included as primary features, encoded and frequency-based versions (e.g., CITY_FREQ, IS_BIG_CITY) were retained to capture any latent regional influence.”"""

# COUNTRY ve CITY frekansları
features_region['COUNTRY_FREQ'] = data['COUNTRY'].map(data['COUNTRY'].value_counts())
features_region['CITY_FREQ'] = data['CITY'].map(data['CITY'].value_counts())

features_region

orders_per_customer = data.groupby(['COUNTRY', 'CUSTOMERNAME']).size().groupby('COUNTRY').mean()

orders_per_customer

features_region['COUNTRY_AVG_ORDERS'] = data['COUNTRY'].map(orders_per_customer)

features_region

country_combo_counts = data_obj.groupby(['COUNTRY', 'PRODUCT_DEAL_COMBO']).size().unstack().fillna(0)
country_combo_ratio = country_combo_counts.div(country_combo_counts.sum(axis=1), axis=0)

country_combo_counts

country_combo_ratio['Large_Vintage Cars'].sort_values(ascending=False).head()

features_region.head()



"""# ***RFM ANALYSIS***"""

# RFM tablosunu oluştur
rfm = data.groupby('CUSTOMERNAME').agg({
    'ORDERDATE': lambda x: (datetime(2020,6,1)- x.max()).days,   # Recency
    'ORDERNUMBER': 'nunique',                              # Frequency
    'SALES': 'sum'                                         # Monetary
}).reset_index()

# Kolonları yeniden adlandır
rfm.columns = ['CUSTOMERNAME', 'Recency', 'Frequency', 'Monetary']

rfm['Recency']

data.groupby('CUSTOMERNAME')['DAYS_SINCE_LASTORDER'].describe()

# Skorlama (en yüksek değere en yüksek skor)
rfm['R_Score'] = 5
rfm['F_Score'] = pd.qcut(rfm['Frequency'], 4, labels=[1, 2, 3, 4])
rfm['M_Score'] = pd.qcut(rfm['Monetary'], 4, labels=[1, 2, 3, 4])

# Skorları sayısal yap
rfm['F_Score'] = rfm['F_Score'].astype(int)
rfm['M_Score'] = rfm['M_Score'].astype(int)

# Kombine skoru string olarak oluştur
rfm['RFM_Score'] =rfm['R_Score'].astype(str) + rfm['F_Score'].astype(str) + rfm['M_Score'].astype(str)

rfm.head()

def segment_fm(row):
    if row['F_Score'] >= 4 and row['M_Score'] >= 4:
        return 'Top Customers'
    elif row['F_Score'] >= 3:
        return 'Loyal'
    elif row['M_Score'] >= 3:
        return 'Big Spenders'
    else:
        return 'Others'

rfm['Segment'] = rfm.apply(segment_fm, axis=1)

rfm

import seaborn as sns
import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

sns.histplot(rfm['Recency'], bins=20, kde=True, ax=axes[0], color='skyblue')
axes[0].set_title('Recency Distribution')

sns.histplot(rfm['Frequency'], bins=20, kde=True, ax=axes[1], color='salmon')
axes[1].set_title('Frequency Distribution')

sns.histplot(rfm['Monetary'], bins=20, kde=True, ax=axes[2], color='lightgreen')
axes[2].set_title('Monetary Distribution')

plt.tight_layout()
plt.show()

sns.countplot(data=rfm, x='Segment', order=rfm['Segment'].value_counts().index, palette='Set2')
plt.title("Customer Segments Count")
plt.ylabel("Number of Customers")
plt.xlabel("Segment")
plt.show()

rfm.groupby('Segment')[['Recency', 'Frequency', 'Monetary']].mean().plot(kind='bar', figsize=(10,6))
plt.title('Average RFM Scores per Segment')
plt.ylabel('Average Value')
plt.xticks(rotation=0)
plt.grid(axis='y')
plt.show()

"""# ***CONCAT FEATURES***"""

features_row_based = pd.concat([
    features_time,
    features_behavior,
    features_region
], axis=1)

features_row_based

features_customer = pd.concat([
    features_customer,  # CUSTOMERNAME zaten index
    rfm.set_index('CUSTOMERNAME')[['F_Score', 'M_Score']],
    customer_combo_matrix # No need to set index again
], axis=1).reset_index()

features_customer

features_behavior

final_features

# 1. features_customer'ı temel al
final_features = features_customer.copy()

# 2. 'CUSTOMERNAME' bilgisini ekle (gerekirse index'ten alınır)
features_row_based = features_row_based.copy()
features_row_based['CUSTOMERNAME'] = data['CUSTOMERNAME']
features_row_based['SALES'] = data['SALES']
features_row_based['QUANTITYORDERED'] = data['QUANTITYORDERED']
features_row_based['DAYS_SINCE_LASTORDER'] = data['DAYS_SINCE_LASTORDER']

# 3. Müşteri bazlı özet istatistikleri oluştur
customer_stats = features_row_based.groupby('CUSTOMERNAME').agg({
    'IS_Q4': 'mean',
    'IS_NOV': 'mean',
    'IS_SUMMER_PEAK': 'mean',
    'IS_CLASSIC_OR_VINTAGE': 'mean',
    'IS_LUXURY': 'mean',
    'IS_TOP_COMBO': 'mean',
    'IS_HEAVY_ORDER': 'mean',
    'IS_HIGH_VALUE_ORDER': 'mean',
    'IS_BIG_CITY': 'mean',
    'IS_BIG_COUNTRY': 'mean',
    'COUNTRY_FREQ': 'mean',
    'CITY_FREQ': 'mean',
    'COUNTRY_AVG_ORDERS': 'mean',
    'SALES': ['mean', 'sum', 'min', 'max'],
    'QUANTITYORDERED': ['mean', 'sum', 'min', 'max'],
    'DAYS_SINCE_LASTORDER': ['mean', 'min', 'max']
})
# Çok seviyeli kolonları düzleştir
customer_stats.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in customer_stats.columns]
customer_stats = customer_stats.reset_index()

final_features = final_features.rename(columns={'index': 'CUSTOMERNAME'})

final_features = final_features.merge(customer_stats, on='CUSTOMERNAME', how='left')
# Sonuçları göster
final_features.head()

final_features.columns

pd.set_option('display.max_columns', None)

final_features

"""# ***REVIEWING FEATURES***"""

import seaborn as sns
import matplotlib.pyplot as plt

# Sadece sayısal sütunları seçiyoruz
numeric_cols = final_features.select_dtypes(include=['int64', 'float64'])

# Korelasyon matrisi hesapla
corr_matrix = numeric_cols.corr()

# Isı haritası
plt.figure(figsize=(40, 25))
sns.heatmap(corr_matrix, cmap='coolwarm', annot=True, fmt=".2f", linewidths=0.5)
plt.title("Korelasyon Matrisi", fontsize=16)
plt.show()

target = 'IS_LUXURY_mean'
correlations = corr_matrix[target].sort_values(ascending=False)
print(f"{target} değişkeni ile korelasyonlar:\n")
print(correlations)

threshold = 0.85  # İstersen 0.9 da olabilir
high_corr = []

for col in corr_matrix.columns:
    for row in corr_matrix.index:
        if col != row and abs(corr_matrix.loc[row, col]) > threshold:
            pair = tuple(sorted((col, row)))
            if pair not in high_corr:
                high_corr.append(pair)

print("Yüksek korelasyonlu değişken çiftleri:")
for pair in high_corr:
    print(f"{pair[0]} ↔ {pair[1]}: {corr_matrix.loc[pair[0], pair[1]]:.2f}")

drop_columns = [
    'SALES_mean', 'SALES_sum', 'SALES_min', 'SALES_max',
    'QUANTITYORDERED_mean', 'QUANTITYORDERED_sum', 'QUANTITYORDERED_min', 'QUANTITYORDERED_max',
    'DAYS_SINCE_LASTORDER_mean', 'DAYS_SINCE_LASTORDER_min', 'DAYS_SINCE_LASTORDER_max',
    'IS_BIG_COUNTRY_mean', 'IS_BIG_CITY_mean', 'COUNTRY_AVG_ORDERS_mean', # IS_BIG_COUNTRY_mean ve IS_BIG_CITY_mean drop ediliyor
    # Tüm PRODUCT_DEAL_COMBO sütunlarını ekleyin
    *customer_combo_matrix.columns # unpacking ile combo matrix sütunlarını drop_columns listesine ekliyoruz.
]
final_features = final_features.drop(columns=drop_columns)

import seaborn as sns
import matplotlib.pyplot as plt

# Sadece sayısal sütunları seçiyoruz
numeric_cols = final_features.select_dtypes(include=['int64', 'float64'])

# Korelasyon matrisi hesapla
corr_matrix = numeric_cols.corr()

# Isı haritası
plt.figure(figsize=(40, 25))
sns.heatmap(corr_matrix, cmap='coolwarm', annot=True, fmt=".2f", linewidths=0.5)
plt.title("Korelasyon Matrisi", fontsize=16)
plt.show()



"""# ***MODEL***"""

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# 1. Sayısal değişkenleri seç
X = final_features.select_dtypes(include=['float64', 'int64']).drop(columns=['IS_TOP_CUSTOMER'])

# 2. Z-score normalizasyon
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 3. Farklı k değerleri için inertia hesapla
inertias = []
k_values = range(1, 11)  # k=1'den 10'a kadar

for k in k_values:
    kmeans = KMeans(n_clusters=k, init='k-means++', n_init=10, random_state=42)
    kmeans.fit(X_scaled)
    inertias.append(kmeans.inertia_)

# 4. Dirsek grafiğini çiz
plt.figure(figsize=(8, 5))
plt.plot(k_values, inertias, marker='o')
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.grid(True)
plt.show()

from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import pandas as pd

# 1. Sayısal değişkenleri al (hedef sütunlar hariç)
X = final_features.select_dtypes(include=['float64', 'int64']).drop(columns=['IS_TOP_CUSTOMER'])

# 2. Z-score standardizasyonu
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 3. PCA: varyansın %95'ini koruyacak şekilde boyut indirgeme
pca = PCA(n_components=0.95, random_state=42)
X_pca = pca.fit_transform(X_scaled)

print(f"PCA ile boyut sayısı: {X_pca.shape[1]}")

# 4. KMeans++ algoritması (k=3)
kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, random_state=42)
kmeans.fit(X_pca)

# 5. Etiketleri dataframe'e ekle
final_features['Cluster_PCA'] = kmeans.labels_

# 6. Silhouette skoru ve küme dağılımı
sil_score = silhouette_score(X_pca, kmeans.labels_)
print("\nSilhouette Score (PCA + KMeans):", sil_score)
print("Küme Dağılımı:")
print(final_features['Cluster_PCA'].value_counts())

# 7. Opsiyonel: PCA bileşenlerine göre dağılım grafiği
plt.figure(figsize=(8,6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans.labels_, cmap='viridis', s=50)
plt.title("PCA Sonrası KMeans Kümeleme (ilk 2 bileşen)", fontsize=14)
plt.xlabel("PCA 1")
plt.ylabel("PCA 2")
plt.grid(True)
plt.colorbar(label='Cluster')
plt.show()

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

# 1. Gerekli verileri ve özellikleri seçin
X = final_features[['F_Score', 'M_Score', 'IS_Q4_mean', 'IS_NOV_mean',
                     'IS_CLASSIC_OR_VINTAGE_mean', 'IS_LUXURY_mean',
                     'IS_HEAVY_ORDER_mean', 'IS_HIGH_VALUE_ORDER_mean',
                     'COUNTRY_FREQ_mean', 'CITY_FREQ_mean']]

# 2. Özellikleri ölçeklendirin
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 3. KMeans modelini eğitin (örneğin, 3 küme)
kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, random_state=42)
kmeans.fit(X_scaled)

# 4. Küme etiketlerini dataframe'e ekleyin
final_features['Cluster'] = kmeans.labels_

# 5. Silhouette skoru ile küme kalitesini değerlendirin
sil_score = silhouette_score(X_scaled, kmeans.labels_)
print("Silhouette Score:", sil_score)

# 6. Kümeleri IS_TOP_CUSTOMER ile ilişkilendirin
# Her küme için IS_TOP_CUSTOMER oranını hesaplayın
cluster_top_customer_ratio = final_features.groupby('Cluster')['IS_TOP_CUSTOMER'].mean()
print("\nKümelerin IS_TOP_CUSTOMER Oranı:\n", cluster_top_customer_ratio)

# 7. Kümeleri yorumlayın ve IS_TOP_CUSTOMER ile ilişkilendirin
# Örneğin, en yüksek IS_TOP_CUSTOMER oranına sahip küme "Top Müşteriler" olarak etiketlenebilir.


from sklearn.cluster import DBSCAN

# DBSCAN modelini oluşturun ve eğitin
dbscan = DBSCAN(eps=0.5, min_samples=5)  # eps ve min_samples parametrelerini ayarlayın
final_features['Cluster_DBSCAN'] = dbscan.fit_predict(X_scaled)

# Kümeleme sonuçlarını değerlendirin
# Check if DBSCAN resulted in more than one cluster
if len(set(final_features['Cluster_DBSCAN'])) > 1:
    sil_score_dbscan = silhouette_score(X_scaled, final_features['Cluster_DBSCAN'])
    print("Silhouette Score (DBSCAN):", sil_score_dbscan)
else:
    print("DBSCAN resulted in only one cluster or noise, silhouette score cannot be calculated.")
# If silhouette score cannot be calculated, adjust DBSCAN parameters or consider alternative clustering methods.



